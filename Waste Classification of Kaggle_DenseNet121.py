# -*- coding: utf-8 -*-
"""Waste Classification KP v9 Kang Fu edited DenseNet121 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d6-7fEpE9-A5uQ_VBcM4MdQQuNkDHLMr
"""

# Improt libraries

import pandas as pd
import numpy as np
import glob
import PIL
from PIL import Image

from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix, classification_report

import matplotlib.pyplot as plt

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv2D, Conv3D, Flatten, MaxPooling2D, AveragePooling2D
from tensorflow.keras.applications import DenseNet121, densenet
from tensorflow.keras import Model
from tensorflow.keras.backend import argmax

# Import google drive and set up the data folder

from google.colab import drive
drive.mount('/content/gdrive')

# Define the train and text data folder address

imdir_train = '/content/gdrive/MyDrive/MMAI 894 Team Project/Train/'
imdir_test = '/content/gdrive/MyDrive/MMAI 894 Team Project/Test/'
img_size = (224, 224)

# Import train and validation images
train_val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    imdir_train,
    label_mode="int",
    subset=None,
    seed=1337,
    image_size=img_size,
    batch_size=None,
    shuffle=False
)

# Import test images
test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    imdir_test,
    label_mode="int",
    subset=None,
    seed=1337,
    image_size=img_size,
    batch_size=None,
    shuffle=False
)

# Function to create a train and val split from a data set object

def get_dataset_partitions_tf(ds, ds_size, train_split=0.625, val_split=0.375, shuffle=True, shuffle_size=10000, batch_size=32):
    assert (train_split + val_split) == 1
    
    if shuffle:
        # Specify seed to always have the same split distribution between runs
        # Note must set reshuffle_each_iteration to False to avoid data leakage
        ds = ds.shuffle(shuffle_size, seed=12, reshuffle_each_iteration = False)
    
    train_size = int(train_split * ds_size)
    val_size = int(val_split * ds_size)
    
    train_ds = ds.take(train_size).batch(batch_size)
    val_ds = ds.skip(train_size).take(val_size).batch(batch_size)
    
    return train_ds, val_ds

# Define the batch size

batch_size = 32

# Get the data set size to calculate the right split
# When batched this is the number of batches not item count

batch_count = train_val_ds.cardinality().numpy()
print(batch_count)

# Split the data into three parts:trian, validation, and test

train_ds, val_ds = get_dataset_partitions_tf(train_val_ds, batch_count, train_split=0.625, 
                                                      val_split=0.375, shuffle=True, shuffle_size=10000, batch_size=batch_size)
# Set test_ds into batches

test_ds = test_ds.batch(batch_size)

# Check that the split worked

print(train_ds.cardinality().numpy())
print(val_ds.cardinality().numpy())
print(test_ds.cardinality().numpy())

# Define a model function

def build_model():

    # DenseNet121 model pre-trained on ImageNet, using AVG pooling at output, freeze training on first 150 layers

    base_model = DenseNet121(include_top=False, weights='imagenet', pooling='avg', input_shape=(224, 224, 3))
    for layer in base_model.layers[:149]:
      layer.trainable = False
    for layer in base_model.layers[149:]:
      layer.trainable = True
    
    input_layer = keras.layers.Input([224,224,3])

    # Use keras' densenet preprocessing to prepare input for DenseNet121 model

    pre_process = keras.applications.densenet.preprocess_input(input_layer)

    x = base_model(pre_process)

    # Fully connected layer with 1024 neurons, 25% dropout

    dense1 = Dense(1024, activation='relu')(x)
    do1 = Dropout(0.25, seed=42)(dense1)

    # Fully connected layer with 1024 neurons, 25% dropout

    dense2 = Dense(1024, activation='relu')(do1)
    do2 = Dropout(0.25, seed=42)(dense2)
    
    # Output layer wiht 5 categories

    output_layer = Dense(5, activation='softmax', kernel_initializer=keras.initializers.GlorotNormal(seed=42))(do2)

    model = Model(input_layer, output_layer)
    model.summary()

    return model

# Define model compile function 

def compile_model(model):

    model.compile(
       # Use sparse catgorical crossentropy loss
       loss=keras.losses.sparse_categorical_crossentropy,

       # Use Adam gradient descent optimizer, set learning rate to 5e-5
       optimizer=keras.optimizers.Adam(learning_rate=5e-5),
       
       # Use accuracy as a metric
       metrics=['accuracy']
    )

    return model

# Define model trainning 

def train_model(model, train, val, epochs = 20):

    # Fit model with train data
    history = model.fit(
        train_ds,
        
        # Use 32 batch size
        batch_size = 32,
        epochs = epochs,
        
        # Verbose training, including validation data
        verbose = 2,
        validation_data = val_ds
    )
    return model, history

# Define model evaluation

def eval_model(model, test):

    # Evaluate model with test data
    test_loss, test_accuracy = model.evaluate(
        test,
        verbose = 2
    )

    return test_loss, test_accuracy

# Finish model trainning and validate the model

model = build_model()
model = compile_model(model)
model, history = train_model(model, train_ds, val_ds)
test_loss, test_accuracy = eval_model(model, test_ds)

# Print the test accuarcy of the model 

print('Test Accuracy:',test_accuracy)

# Extract the labels from the test data set

Y_test_class = tf.concat([labels for images, labels in test_ds], axis=0)

# Pass the test set directly to the predict function

Y_pred_class = argmax(model.predict(test_ds))

# Import libraries for EDA

import seaborn as sb
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Setting default fig size

size_ = (10,8)

# Generate Confusion Matrix for the 5 Classes : black bin, blue bin, donate drop-offï¼Œ green bin, and hazardous waste

cm=confusion_matrix(Y_test_class,Y_pred_class)
df_cm = pd.DataFrame(cm, range(cm.shape[0]), range(cm.shape[1]))
df_cm.columns = train_val_ds.class_names
df_cm.index = train_val_ds.class_names

fig = plt.subplots(figsize=size_)
sb.set(font_scale=1)
sb.heatmap(df_cm, annot=True, annot_kws={"size": 10}, cmap="Blues") # font size
plt.show()

# Generate Classification Report for the 5 Classes

print(classification_report(Y_test_class,Y_pred_class))

# Narrow down to only recyclable and non-recyclable bins
# 0 = non-recyclable
# 1 = recycleable

def recycle_classify(labels):
  y = np.copy(labels)
  waste_index = [0,1,0,0,0]
  for i in range(5):
    y = np.where(y == i, waste_index[i], y)
  return y

# Change the label to narrowed 0 and 1 

Y_recycle_class_pred = recycle_classify(Y_pred_class)
Y_recycle_class_test = recycle_classify(Y_test_class)

# Generate Confusion Matrix for Recycleable vs. Non-Recyclable

cm=confusion_matrix(Y_recycle_class_test,Y_recycle_class_pred)
df_cm = pd.DataFrame(cm, range(cm.shape[0]), range(cm.shape[1]))
df_cm.columns = ['non-recyclable','recyclable']
df_cm.index = ['non-recyclable','recyclable']

fig = plt.subplots(figsize=size_)
sb.set(font_scale=1)
sb.heatmap(df_cm, annot=True, annot_kws={"size": 10}, cmap="Blues") # font size
plt.show()

# Generate Classification Report for Recycleable vs. Non-Recyclable

print(classification_report(Y_recycle_class_test,Y_recycle_class_pred))

